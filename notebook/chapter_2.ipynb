{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35534557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc36b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 -> allocated: 0.00 GB | reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def clear_memory(extra_names=()):\n",
    "    \"\"\"\n",
    "    Aggressively free GPU memory across *all* CUDA devices.\n",
    "    Pass any extra global var names via extra_names if needed.\n",
    "    \"\"\"\n",
    "    # 0) Early exit if no CUDA\n",
    "    if not torch.cuda.is_available():\n",
    "        gc.collect()\n",
    "        return print(\"CUDA not available. Collected CPU garbage only.\")\n",
    "\n",
    "    # 1) Try to move known models off GPU (if still around)\n",
    "    for name in (\"model\", \"base_model\"):\n",
    "        obj = globals().get(name, None)\n",
    "        try:\n",
    "            if obj is not None and hasattr(obj, \"to\"):\n",
    "                obj.to(\"cpu\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) Delete common globals (if present)\n",
    "    for name in (\"inputs\", \"base_model\", \"model\", \"tokenizer\", *extra_names):\n",
    "        globals().pop(name, None)\n",
    "\n",
    "    # 3) Delete any stray CUDA tensors lingering in globals()\n",
    "    for name, obj in list(globals().items()):\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                del globals()[name]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Full GC pass\n",
    "    gc.collect()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # 5) Clear *each* CUDA device\n",
    "    for idx in range(torch.cuda.device_count()):\n",
    "        try:\n",
    "            with torch.cuda.device(idx):\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.empty_cache()\n",
    "                # Collect interprocess memory (helps when using multiple processes / dataloaders)\n",
    "                torch.cuda.ipc_collect()\n",
    "                # Reset peak stats (optional, for cleaner diagnostics)\n",
    "                try:\n",
    "                    torch.cuda.reset_peak_memory_stats(idx)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            # Keep going even if one device throws\n",
    "            pass\n",
    "\n",
    "    # 6) One more GC + sync\n",
    "    gc.collect()\n",
    "    for idx in range(torch.cuda.device_count()):\n",
    "        try:\n",
    "            with torch.cuda.device(idx):\n",
    "                torch.cuda.synchronize()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 7) Report per-device\n",
    "    for idx in range(torch.cuda.device_count()):\n",
    "        alloc = torch.cuda.memory_allocated(idx) / (1024 ** 3)\n",
    "        reserv = torch.cuda.memory_reserved(idx) / (1024 ** 3)\n",
    "        print(f\"cuda:{idx} -> allocated: {alloc:.2f} GB | reserved: {reserv:.2f} GB\")\n",
    "\n",
    "# run it\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f6acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.81s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ccbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>Here is an email apologizing to Sarah for the tragic gardening mishap:\n",
      "\n",
      "Subject: A Sinc\n",
      "tensor([[128000,   8144,    459,   2613,  21050,   4954,    311,  21077,    369,\n",
      "            279,  35279,  60299,  64496,    391,     13,  83017,   1268,    433,\n",
      "           7077,  16134,     91,  78191,     91,     29]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=20\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b893e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Write\n",
      " an\n",
      " email\n",
      " apolog\n",
      "izing\n",
      " to\n",
      " Sarah\n",
      " for\n",
      " the\n",
      " tragic\n",
      " gardening\n",
      " mish\n",
      "ap\n",
      ".\n",
      " Explain\n",
      " how\n",
      " it\n",
      " happened\n",
      ".<\n",
      "|\n",
      "assistant\n",
      "|\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "for id in input_ids[0]:\n",
    "   print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a441b3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   8144,    459,   2613,  21050,   4954,    311,  21077,    369,\n",
       "            279,  35279,  60299,  64496,    391,     13,  83017,   1268,    433,\n",
       "           7077,  16134,     91,  78191,     91,     29,   8586,    374,    459,\n",
       "           2613,  21050,   4954,    311,  21077,    369,    279,  35279,  60299,\n",
       "          64496,    391,   1473,  13317,     25,    362,    328,   2910]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
